{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias in Data\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This project explores the concept of *bias* by examinging how the number and quality of Wikipedia articles about political figures vary among countries.\n",
    "\n",
    "Several specific questions are addressed:\n",
    "- Which countries have the greatest and the least coverage of politicians on Wikipedia compared to their population?\n",
    "- which countries have the highest and lowest proportion of high quality articles about politicians?\n",
    "- Which regions have the most articles about politicians, relative to their populations?\n",
    "- Which regions have teh highest proprtion of high-quality articles about politicians?\n",
    "\n",
    "Article quality is estimated using a machine learning service called ORES. NOTES ABOUT PACKAGES AND THINGS THAT ARE USED TO CONDUCT ANALYSIS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion and Cleaning\n",
    "\n",
    "### Data Sources\n",
    "The data used in this analysis is drawn from two sources:\n",
    "- The Wikipedia politicians by country dataset, found on Figshare: https://figshare.com/articles/Untitled_Item/5513449\n",
    "- A subset of the world population datasheet published by the Population Reference Bureau\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "The Wikipedia *Politicians by Country* dataset contains some pages which are not Wikipedia articles. These pages are filtered out before we conduct our analysis by removing all page names that begin with the string \"Template:\".\n",
    "\n",
    "The Population Reference Bureau *World Population Datasheet* contains some rows relating to regional population counts. These are filtered out prior to country-level analyses performed below, but utilized in the final two tables in the Analysis section and in the Reflection section to address coverage and quality by region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed packages\n",
    "import pandas as pd\n",
    "\n",
    "# read the csv files in to Pandas data frames\n",
    "politicos_full = pd.read_csv(\"page_data.csv\")\n",
    "pops_regions = pd.read_csv(\"WPDS_2018_data.csv\")\n",
    "# check that the imports have worked correctly\n",
    "#print(politicos_full.head())\n",
    "\n",
    "# remove the no-Wikipedia articles by filtering the politicos data frame to remove instances of the string \"Template:\"\n",
    "politicos = politicos_full[~politicos_full.page.str.contains(\"Template:\")]\n",
    "# check that the filtering step has worked correctly\n",
    "#print(politicos.head())\n",
    "\n",
    "# remove the regions from the population data frame by removing rows where the geography col is all caps\n",
    "# first we make a deep copy of the dataframe because we want a dataframe free of regions, but we also want the region data\n",
    "pops_countries = pops_regions.copy(deep=True)\n",
    "# drop regions from the new countries dataframe for the upcoming analysis\n",
    "pops_countries.drop(pops_countries[pops_countries['Geography'].str.isupper()].index, inplace = True)\n",
    "# drop countries from the regions dataframe so the two will be completely distinct\n",
    "#pops_regions = pops_regions[pops_regions['Geography'].str.isupper()]\n",
    "\n",
    "# check that both dataframes are correct\n",
    "#print(pops_regions.head())\n",
    "#print(pops_countries.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Predictions\n",
    "\n",
    "In the following code we use the ORES API to get json files which contain predictions about the quality of individual articles.\n",
    "\n",
    "ORES documentation: https://ores.wikimedia.org/v3/#!/scoring/get_v3_scores_context\n",
    "\n",
    "There are six total quality categories. The first two categories (FA and GA) are considered high quality.\n",
    "\n",
    "FA - Featured article\n",
    "GA - Good article\n",
    "B - B-class article\n",
    "C - C-class article\n",
    "Start - Start-class article\n",
    "Stub - Stub-class article\n",
    "\n",
    "The first fuction in the following code get_ores_data is taken from https://github.com/Ironholds/data-512-a2/blob/master/hcds-a2-bias_demo.ipynb and modified only so that it returns the result (rather than simply printing it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed packages\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# this block of code is taken from https://github.com/Ironholds/data-512-a2/blob/master/hcds-a2-bias_demo.ipynb\n",
    "# it is modified only so that get_ores_data returns the result response\n",
    "headers = {'User-Agent' : 'https://github.com/chisquareatops', 'From' : 'hertman@uw.edu'}\n",
    "\n",
    "def get_ores_data(revision_ids, headers):    \n",
    "    # Define the endpoint\n",
    "    endpoint = 'https://ores.wikimedia.org/v3/scores/{project}/?models={model}&revids={revids}'    \n",
    "    # Specify the parameters - smushing all the revision IDs together separated by | marks.\n",
    "    # Yes, 'smush' is a technical term, trust me I'm a scientist.\n",
    "    # What do you mean \"but people trusting scientists regularly goes horribly wrong\" who taught you tha- oh.  \n",
    "    params = {'project' : 'enwiki',\n",
    "              'model'   : 'wp10',\n",
    "              'revids'  : '|'.join(str(x) for x in revision_ids)\n",
    "              }\n",
    "    api_call = requests.get(endpoint.format(**params))\n",
    "    response = api_call.json()\n",
    "    #print(json.dumps(response, indent=4, sort_keys=True\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to extract the overall prediction from the above function, which also returns sores for all page types\n",
    "\n",
    "# make a list of the ids\n",
    "revids = list(politicos['rev_id'])\n",
    "\n",
    "# loop through the list of ids in chunks of 100\n",
    "def get_pred(df, block_size): \n",
    "    start = 0\n",
    "    end = block_size\n",
    "    output_final = list()\n",
    "    while start < len(revids):\n",
    "        revids_temp = revids[start:end]\n",
    "        output_temp = get_ores_data(revids_temp, headers)\n",
    "        for key, item in output_temp['enwiki']['scores'].items():\n",
    "            dict_temp = dict()\n",
    "            dict_temp['rev_id'] = key\n",
    "            if 'error' in item['wp10']:\n",
    "                dict_temp['prediction'] = 'no score'\n",
    "            else:\n",
    "                dict_temp['prediction'] = item['wp10']['score']['prediction']\n",
    "            output_final.append(dict_temp)\n",
    "        start += 100\n",
    "        end += 100\n",
    "    scores = pd.DataFrame(output_final)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the above functions to get the predictions for our data frame; divide the articles into blocks of 100\n",
    "politicos_preds = get_pred(politicos, 100)\n",
    "\n",
    "# check that the above step worked correctly\n",
    "#print(politicos_preds.head())\n",
    "\n",
    "# save the articles with no score to a csv and then remove them from the data frame\n",
    "pred[pred.prediction == 'no score'][['rev_id']].to_csv('wp_wpds_articles-no_score.csv')\n",
    "politicos_preds = politicos_preds[~politicos_preds.prediction.str.contains(\"no score\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge and Output Data\n",
    "\n",
    "In the following code we merge our data so that the predictions we are interested in are associated with the individual articles in our data set. We then export a csv of this combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           article_name country  revision_id article_quality Geography  \\\n",
      "0        Bir I of Kanem    Chad  355319463.0            Stub      Chad   \n",
      "1  Abdullah II of Kanem    Chad  498683267.0            Stub      Chad   \n",
      "2   Salmama II of Kanem    Chad  565745353.0            Stub      Chad   \n",
      "3       Kuri I of Kanem    Chad  565745365.0            Stub      Chad   \n",
      "4   Mohammed I of Kanem    Chad  565745375.0            Stub      Chad   \n",
      "\n",
      "  population  \n",
      "0       15.4  \n",
      "1       15.4  \n",
      "2       15.4  \n",
      "3       15.4  \n",
      "4       15.4  \n"
     ]
    }
   ],
   "source": [
    "# make copies just in case before merging\n",
    "politicos_final = politicos.copy(deep=True)\n",
    "politicos_preds_final = politicos_preds.copy(deep=True)\n",
    "pops_countries_final = pops_countries.copy(deep=True)\n",
    "\n",
    "# merge the politcal article data and the quality predictions on the rev_id/revision_id cols\n",
    "politicos_preds_final = politicos_preds_final.astype({'rev_id': 'int64'})\n",
    "combined_final = politicos_final.merge(politicos_preds_final, how='right', left_on='rev_id', right_on='rev_id')\n",
    "# merge the new data frame with the population data on the country/Geography cols\n",
    "combined_final = combined_final.merge(pops_countries_final, how='right', left_on='country', right_on='Geography')\n",
    "\n",
    "# check that the above step worked\n",
    "#print(combined_final.head())\n",
    "\n",
    "# rename the cols to comply with assignment\n",
    "combined_final.rename(columns={'page':'article_name','Population mid-2018 (millions)':'population','rev_id':'revision_id','prediction':'article_quality'}, inplace=True)\n",
    "\n",
    "# save the rows that have no match on the country field to a csv, then drop from the final data frame\n",
    "combined_final[combined_final.Geography.isnull()].to_csv('wp_wpds_countries-no_match.csv')\n",
    "combined_final.dropna(inplace=True)\n",
    "\n",
    "# remove Geography col to comply with assigment (now that rows with no country match are gone)\n",
    "combined_final.drop('Geography', axis=1)\n",
    "\n",
    "# check that the above step worked\n",
    "print(combined_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change some data types so the following analysis will work\n",
    "combined_final['population'] = combined_final['population'].str.replace(',', '')\n",
    "combined_final = combined_final.astype({'population':'float'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "In this section we create the following six individual tables:\n",
    "\n",
    "- Top 10 countries by coverage: 10 highest-ranked countries in terms of number of politician articles as a proportion of country population\n",
    "- Bottom 10 countries by coverage: 10 lowest-ranked countries in terms of number of politician articles as a proportion of country population\n",
    "- Top 10 countries by relative quality: 10 highest-ranked countries in terms of the relative proportion of politician articles that are of GA and FA-quality\n",
    "- Bottom 10 countries by relative quality: 10 lowest-ranked countries in terms of the relative proportion of politician articles that are of GA and FA-quality\n",
    "- Geographic regions by coverage: Ranking of geographic regions (in descending order) in terms of the total count of politician articles from countries in each region as a proportion of total regional population\n",
    "- Geographic regions by coverage: Ranking of geographic regions (in descending order) in terms of the relative proportion of politician articles from countries in each region that are of GA and FA-quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select for high quality articles by keeping only the FA and GA designations in the article_quality field\n",
    "combined_final_2 = combined_final.copy(deep=True)\n",
    "hq_articles = combined_final_2.loc[combined_final_2['article_quality'].isin(['FA','GA'])]\n",
    "# count total number of high quality articles in each country using group by \n",
    "hq_articles_country = hq_articles.groupby('country').count()['article_name']\n",
    "\n",
    "# make this result into a dataframe with appropriate cols so we can bring back population data and report the proportion\n",
    "hq_articles_country_df = hq_articles_country.to_frame()\n",
    "hq_articles_country_df['country'] = hq_articles_country_df.index\n",
    "hq_articles_country_df.reset_index(drop=True, inplace=True)\n",
    "hq_articles_country_df = hq_articles_country_df.merge(pops_countries_final, how='inner', left_on='country', right_on='Geography')\n",
    "\n",
    "# find the actual proprtion: divide number of high quality articles by total population\n",
    "hq_articles_country_df = hq_articles_country_df.astype({'article_name': 'float'})\n",
    "hq_articles_country_df['Population mid-2018 (millions)'] = hq_articles_country_df['Population mid-2018 (millions)'].str.replace(',', '')\n",
    "hq_articles_country_df = hq_articles_country_df.astype({'Population mid-2018 (millions)': 'float'})\n",
    "hq_articles_country_df['article_proportion'] = hq_articles_country_df['article_name'] / (hq_articles_country_df['Population mid-2018 (millions)'] * 1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 countries by coverage: 10 highest-ranked countries in terms of number of politician articles as a proportion of country population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         country  article_proportion\n",
      "130       Tuvalu            0.000500\n",
      "33      Dominica            0.000014\n",
      "46       Grenada            0.000010\n",
      "137      Vanuatu            0.000010\n",
      "52       Iceland            0.000005\n",
      "57       Ireland            0.000004\n",
      "13        Bhutan            0.000004\n",
      "79      Maldives            0.000003\n",
      "90   New Zealand            0.000002\n",
      "58        Israel            0.000002\n"
     ]
    }
   ],
   "source": [
    "# sort by proportion and display a table of the top 10\n",
    "articles_over_pop = hq_articles_country_df[['country','article_proportion']]\n",
    "articles_over_pop = articles_over_pop.sort_values('article_proportion', ascending=False)\n",
    "print(articles_over_pop.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bottom 10 countries by coverage: 10 highest-ranked countries in terms of number of politician articles as a proportion of country population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        country  article_proportion\n",
      "93      Nigeria        1.020929e-08\n",
      "53        India        1.239700e-08\n",
      "125    Tanzania        1.692047e-08\n",
      "9    Bangladesh        1.802885e-08\n",
      "38     Ethiopia        1.860465e-08\n",
      "27     Colombia        2.008032e-08\n",
      "17       Brazil        2.865330e-08\n",
      "26        China        2.941599e-08\n",
      "99         Peru        3.105590e-08\n",
      "88        Nepal        3.367003e-08\n"
     ]
    }
   ],
   "source": [
    "#Bottom 10 countries by coverage\n",
    "articles_over_pop = articles_over_pop.sort_values('article_proportion', ascending=True)\n",
    "print(articles_over_pop.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 countries by relative quality\n",
    "\n",
    "# group the same way we did in previous steps, but this time using all articles\n",
    "all_articles = combined_final.loc[combined_final['article_quality'].isin(['FA','GA','B','C','Start','Stub'])]\n",
    "# count total number of high quality articles in each country using group by \n",
    "all_articles_country = all_articles.groupby('country').count()['article_name']\n",
    "\n",
    "# make a dataframe with this total number of articles per country so it can be merged with dataframe from prev step\n",
    "all_articles_country_df = all_articles_country.to_frame()\n",
    "all_articles_country_df['country'] = all_articles_country_df.index\n",
    "all_articles_country_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "all_articles_country_df = all_articles_country_df.astype({'article_name': 'float'})\n",
    "all_articles_country_df.rename(columns = {'article_name':'total_articles'}, inplace = True) \n",
    "all_articles_country_df = all_articles_country_df.merge(hq_articles_country_df, how='right', left_on='country', right_on='country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 countries by relative quality: 10 highest-ranked countries in terms of the relative proportion of politician articles that are of GA and FA-quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      country  quality_to_total\n",
      "64               Korea, North          0.194444\n",
      "107              Saudi Arabia          0.127119\n",
      "81                 Mauritania          0.125000\n",
      "23   Central African Republic          0.121212\n",
      "104                   Romania          0.113703\n",
      "130                    Tuvalu          0.092593\n",
      "13                     Bhutan          0.090909\n",
      "33                   Dominica          0.083333\n",
      "122                     Syria          0.078125\n",
      "12                      Benin          0.076923\n"
     ]
    }
   ],
   "source": [
    "# find the proprtion: divide number of high quality articles by total articles\n",
    "all_articles_country_df['quality_to_total'] = all_articles_country_df['article_name'] / all_articles_country_df['total_articles']\n",
    "hqarticles_over_total = all_articles_country_df[['country','quality_to_total']]\n",
    "hqarticles_over_total = hqarticles_over_total.sort_values('quality_to_total', ascending=False)\n",
    "print(hqarticles_over_total.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bottom 10 countries by relative quality: 10 highest-ranked countries in terms of the relative proportion of politician articles that are of GA and FA-quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         country  quality_to_total\n",
      "11       Belgium          0.001923\n",
      "125     Tanzania          0.002469\n",
      "121  Switzerland          0.002488\n",
      "88         Nepal          0.002801\n",
      "99          Peru          0.002857\n",
      "93       Nigeria          0.002954\n",
      "27      Colombia          0.003509\n",
      "74     Lithuania          0.004098\n",
      "39          Fiji          0.005076\n",
      "7     Azerbaijan          0.005587\n"
     ]
    }
   ],
   "source": [
    "hqarticles_over_total = hqarticles_over_total.sort_values('quality_to_total', ascending=True)\n",
    "print(hqarticles_over_total.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regions by coverage\n",
    "# The only data source we have that connects countries to regions is the original WPDS_2018_data.csv data (now pops_regions)\n",
    "# countries in this data belong to the region that precedes them in the file, so we need to loop through it.\n",
    "\n",
    "# create an empty dict to hold country/region pairs as we find them\n",
    "region_dict = {}\n",
    "\n",
    "# loop through the original data we preserved (as a list) to identify countries vs. regions, then store pairs\n",
    "for value in pops_regions['Geography'].tolist():\n",
    "    # if the current row is a region, make it the current region (the first row is a region)\n",
    "    if value.isupper():\n",
    "        region = value\n",
    "    # if the current row is a country, add a new country/region pair to the dict\n",
    "    else:\n",
    "        region_dict.update({value:region})\n",
    "\n",
    "# use a lambda to make a new col in the most recent dataframe and use the dict to insert a region value\n",
    "all_articles_country_df['region'] = all_articles_country_df['country'].apply(lambda x: region_dict[x])\n",
    "\n",
    "# test that the above step worked correctly\n",
    "#print(all_articles_country_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geographic regions by coverage: Ranking of geographic regions (in descending order) in terms of the total count of politician articles from countries in each region as a proportion of total regional population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            region  total_articles_over_pop\n",
      "5                          OCEANIA                72.668561\n",
      "2                           EUROPE                19.907834\n",
      "3  LATIN AMERICA AND THE CARIBBEAN                 7.932139\n",
      "0                           AFRICA                 5.867868\n",
      "4                 NORTHERN AMERICA                 5.260131\n",
      "1                             ASIA                 2.544333\n"
     ]
    }
   ],
   "source": [
    "# add up the total number of articles in each region using group by \n",
    "all_articles_region = all_articles_country_df.groupby('region').sum()['total_articles']\n",
    "\n",
    "# turn that result back into a data frame\n",
    "all_articles_region_df = all_articles_region.to_frame()\n",
    "all_articles_country_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# add up the total number of articles in each region using group by \n",
    "pop_region = all_articles_country_df.groupby('region').sum()['Population mid-2018 (millions)']\n",
    "\n",
    "# turn that result back into a data frame\n",
    "pop_region_df = pop_region.to_frame()\n",
    "pop_region_df.reset_index(inplace=True)\n",
    "\n",
    "#all_articles_region_df = all_articles_region_df.sort_values('article_proportion', ascending=False)\n",
    "\n",
    "all_articles_over_pop = all_articles_region_df.merge(pop_region_df, how='right', left_on='region', right_on='region')\n",
    "all_articles_over_pop['total_articles_over_pop'] = all_articles_over_pop['total_articles']/all_articles_over_pop['Population mid-2018 (millions)']\n",
    "all_articles_over_pop = all_articles_over_pop[['region', 'total_articles_over_pop']]\n",
    "all_articles_over_pop = all_articles_over_pop.sort_values('total_articles_over_pop', ascending=False)\n",
    "\n",
    "print(all_articles_over_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geographic regions by coverage: Ranking of geographic regions (in descending order) in terms of the relative proportion of politician articles from countries in each region that are of GA and FA-quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            region  hq_over_all_articles\n",
      "4                 NORTHERN AMERICA              0.051536\n",
      "1                             ASIA              0.027143\n",
      "5                          OCEANIA              0.023462\n",
      "2                           EUROPE              0.022587\n",
      "0                           AFRICA              0.021324\n",
      "3  LATIN AMERICA AND THE CARIBBEAN              0.014002\n"
     ]
    }
   ],
   "source": [
    "# add up the total number of articles in each region using group by \n",
    "all_articles_region = all_articles_country_df.groupby('region').sum()['total_articles']\n",
    "\n",
    "# turn that result back into a data frame\n",
    "all_articles_region_df = all_articles_region.to_frame()\n",
    "all_articles_country_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# add up the total number of high quality articles in each region using group by \n",
    "hq_region = all_articles_country_df.groupby('region').sum()['article_name']\n",
    "\n",
    "# turn that result back into a data frame\n",
    "hq_region_df = hq_region.to_frame()\n",
    "hq_region_df.reset_index(inplace=True)\n",
    "\n",
    "hq_over_all_articles_df = all_articles_region_df.merge(hq_region_df, how='right', left_on='region', right_on='region')\n",
    "\n",
    "hq_over_all_articles_df['hq_over_all_articles'] = hq_over_all_articles_df['article_name']/hq_over_all_articles_df['total_articles']\n",
    "hq_over_all_articles_df = hq_over_all_articles_df[['region', 'hq_over_all_articles']]\n",
    "hq_over_all_articles_df = hq_over_all_articles_df.sort_values('hq_over_all_articles', ascending=False)\n",
    "\n",
    "print(hq_over_all_articles_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "Potentially the most significant source of potential bias is this analysis is the ORES scores themselves and the way in which they are generated. Unfortunately I can't speak to this as I don't know how the detailed algorithms behind this scores. I can only state that I cannot be sure whether these scores thoroughly account for possible cultural differences when evaluating the 'quality' of an article.\n",
    "\n",
    "Another potential problem with the data is different political structures in different countries. Governments can vary widely in size and different branches of government vary wiely in how much power they weild, how long they are in office, and other important measures both within and between countries. Therefore, it is possible that one country may have proportionaly more political offices which warrant (and require) lengthy or high quality explanation than another country.\n",
    "\n",
    "The tables generated here suggest possible hypothesis about internet access: it's possible that countries where a higher percentage of the population has internet access will tend to have more total articles and proportionally more high-quality articles. This would have to be tested by brining in additional data. It would also be interesting to combine the above data set with national GDP data to investigate any possible correlations with wealth at the national or regional level."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
